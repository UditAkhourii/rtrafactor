# -*- coding: utf-8 -*-
"""Rtrafactor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nzJou9tONTF6AOKqJTZc8--rIwI38JuR

# Welcome to Eternity AI's Colaboratory.

This is `Rtrafactor` workspace. Use this to add RTRA or Real Time Retrival Argumentation architecture into your Huggingface model.

RTRA is a research project at Indian Institute of Technology, Patna. It follows an alternate language model architecture to connect real-time access tools like search, retrieval , summarization and argumentation into your AI model by our Python library.

To read our research paper, please click [here](https://eternityai.tech).

# Step #1 Library Installation

Install `Rtrafactor` library into your project.
"""

pip install rtrafactor

"""# Step #2 Package Validation

Validate package's authenticity. Please make sure that you install the latest version of Rtrafactor and check upon updates regularly from [here](https://pypi.org/project/rtrafactor/).
"""

pip show rtrafactor

"""# Step #3 Import RTRAConnector

Rtrafactor is a standalone package that runs on a RTRAConnector class. Make sure you import it before integrating Rtrafactor in your project.
"""

from rtrafactor import RTRAConnector

"""# Step #4 Usage
Package installation, verification and import is successfully completed so far. This is the time to give superpowers to your language model by connecting it with Rtraconnector.

NOTE : Rtrafactor is currently available as a L2LM architecture (more details in rearch paper). This means, it can integrate with pre-existing models that are deployed on HuggingFace's Inference API. At this moment, we are still working on Rtrafactor to make it multimodaal and be able to connect with any model, even outside HuggingFace.

To use `Rtrafactor`, you will need your HuggingFace token and model. API Tokens can be generated from [here](https://huggingface.co/settings/tokens). Make sure you give `write access` to your token.

You can also use any publicly available or custom model that is available on HuggingFace's Inference API.

# Example #1
Why is Delhi's CM in jail?
"""

# Example usage:
huggingface_model = "username/huggingface_model"
huggingface_api_token = "write_api_token"

connector = RTRAConnector(huggingface_model, huggingface_api_token)
query = "Why is Delhi's CM in jail?"
one_shot_answer = connector.compare_answers(query)
print(one_shot_answer)

"""# Example #2
Who is Dr. Kuldip Singh Patel?
"""

# Example usage:
huggingface_model = "username/huggingface_model"
huggingface_api_token = "write_api_token"

connector = RTRAConnector(huggingface_model, huggingface_api_token)
query = "Who is Dr. Kuldip Singh Patel?"
one_shot_answer = connector.compare_answers(query)
print(one_shot_answer)

"""# Example #3

Who is Udit Akhouri?
"""

# Example usage:
huggingface_model = "username/huggingface_model"
huggingface_api_token = "write_api_token"


connector = RTRAConnector(huggingface_model, huggingface_api_token)
query = "Who is Udit Akhouri?"
one_shot_answer = connector.compare_answers(query)
print(one_shot_answer)

"""# Limitations & Future Scope of this research

The RTRA architecture enhances the model's capabilities. However, the model might still face hallucination challenges. The model operates on a one-shot response retrieval algorithm which although increases precision, has also a lot of scope for improvements.

Here are some more limitations that we are solving in rtrafactor :     

1.   Hallucination & citation : This is easy, if you go to console, you will see all the web pages it has scraped to prepare the context for the response. We can use a two-shot retrival argumentation to let the machine argue which info is in the context and which one is not.
2.   Latency : The most significant problem that we will be focusing in our Sem3 & Sem4 will be reducing latency. Currently, we agree that there is a lot of room for optimisation of the underlying architecture and we are not ignoring that. However, hallucination and latency are the evil twins of each other. If we have to reduce hallucination, we need to scrape more resourceful content which means more underlying processes running and higher latency to occur.

# Why is this a breakthrough?
The most significant question that we were asking ourselves while conducting this research was that why are we even building this?
Well, the answer is a breakthrough in how we build LLMs in the first place.

Currently, we are in the midst of a race of who can scrape the most of the interent and put it on their servers so their model can answer it. This requires a lot of computation and makes it impossible for solo developers and small teams to make a super efficient model.

RTRA architecture challenges this convection thought process. We belive that the interent is available for free and anyone can access it. That's why, you don't need to rely on huge computational resources (unline what Jansen Huang or Sam Altman says).

We are not on a war with Open AI, Nvidia or Google. However, we are just democratizing the resources so even a 1st year undergrad student can build models as efficient as GPT4 right from his Google Colab notebook.

We are currently testing our RTRA on low-parameter models and have successfully increased efficiency of Google's Gemma 7B to best Gemini 1.0 in multiple precision benchmarks. We are planning work specifically on models with <1 billion parameters to achieve the same efficiency of atleast a 7 billion parameter model with the help of RTRA architecture.
"""